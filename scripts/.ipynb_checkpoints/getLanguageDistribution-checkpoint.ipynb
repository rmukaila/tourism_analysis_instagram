{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import chart_studio.plotly as py\n",
    "import json\n",
    "import operator\n",
    "from langdetect import detect, detect_langs\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "province = 'antalya'\n",
    "preprocessing_dir = '/home/emotionlex/Documents/newest/instaturkeydata/PREPROCESSING/'\n",
    "en_ids = open(preprocessing_dir+province+\"_en_ids.txt\", \"a\")\n",
    "tr_ids = open(preprocessing_dir+province+\"_tr_ids.txt\", \"a\")\n",
    "# ca_ids = open(\"../../../ssd2/instaBarcelona/ca_ids.txt\", \"w\")\n",
    "out_file = open('/home/emotionlex/Documents/newest/instaturkeydata/'+province+\"_lang_data.json\", 'w')\n",
    "out_file_all = open('/home/emotionlex/Documents/newest/instaturkeydata/'+province+\" _lang_data_all.json\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Counting languages\n"
     ]
    }
   ],
   "source": [
    "# omited_words_file = open(\"../../../ssd2/instaBarcelona/words_2_filter.txt\", \"r\")\n",
    "# omited_words = []\n",
    "# for line in omited_words_file:\n",
    "#     w = unicode(line.rstrip(), \"utf-8\")\n",
    "#     omited_words.append(w)\n",
    "  \n",
    "\n",
    "print (\"Loading data\")\n",
    "with open(\"/home/emotionlex/Documents/newest/instaturkeydata/JSONS/\"+province+'_extracted_captions.json', \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print (\"Counting languages\")\n",
    "languages = {'en': 0, 'tr': 0}\n",
    "all_languages = {}\n",
    "en = []\n",
    "tr = []\n",
    "# ca = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(k, v):\n",
    "    cap_lan = \"unknown\"\n",
    "    caption = v.replace('#', ' ')\n",
    "    caption = caption.replace('\\n', ' ')\n",
    "    caption = caption.rstrip()\n",
    "    caption = caption.lower()\n",
    "\n",
    "    # for word2omit in omited_words:\n",
    "    #     if word2omit in caption:\n",
    "    #         print caption\n",
    "    #         caption = caption.replace(word2omit, ' ')\n",
    "    #         print caption\n",
    "\n",
    "    it = 0\n",
    "    while cap_lan == \"unknown\":  # Using while because the LSTM gives different answers, and we are supposed to have fitlered other languages\n",
    "        try:\n",
    "            langs = detect_langs(caption)\n",
    "            for cur_lan in langs:\n",
    "                cur_lan_str = str(cur_lan).split(':')[0]\n",
    "                # Assign to one of the used languages\n",
    "                if cur_lan_str in languages.keys():\n",
    "                    cap_lan = cur_lan_str\n",
    "                    break\n",
    "        except:\n",
    "            print (\"Lang detection failed. Continuing\")\n",
    "\n",
    "        it += 1\n",
    "        if it == 10:\n",
    "            # print \"Limit of iterations reached. Continuing\"\n",
    "            break\n",
    "\n",
    "    # if cap_lan is 'unknown':\n",
    "    #    print \"Lang not found\"\n",
    "\n",
    "    # Save language with higher prob\n",
    "    try:\n",
    "        cap_lan_higher_prob = str(langs[0]).split(':')[0]\n",
    "    except:\n",
    "        cap_lan_higher_prob = \"unknown\"\n",
    "\n",
    "    return k + ',' + cap_lan + ',' + cap_lan_higher_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelizer = Parallel(n_jobs=12)\n",
    "tasks_iterator = (delayed(detect_lang)(k, v) for k, v in data.items())\n",
    "r = parallelizer(tasks_iterator)\n",
    "# merging the output of the jobs\n",
    "strings = np.vstack(r)\n",
    "print (\"Computing done, geting results...\")\n",
    "\n",
    "dicarded = 0\n",
    "for r in strings:\n",
    "    k = r[0].split(',')[0]\n",
    "    cap_lan = r[0].split(',')[1]\n",
    "    cap_lan_higher = r[0].split(',')[2]\n",
    "    if cap_lan == 'unknown':\n",
    "        print (\"Unknown language\")\n",
    "        dicarded += 1\n",
    "        continue\n",
    "    if cap_lan == 'en':\n",
    "        en.append(k)\n",
    "    elif cap_lan == 'tr':\n",
    "        tr.append(k)\n",
    "    # elif cap_lan == 'ca':\n",
    "    #     ca.append(k)\n",
    "    languages[cap_lan] += 1\n",
    "\n",
    "    if cap_lan_higher in all_languages:\n",
    "        all_languages[cap_lan_higher] += 1\n",
    "    else:\n",
    "        all_languages[cap_lan_higher] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"SELECTED LANGUAGES\")\n",
    "print (languages)\n",
    "print (\"Number of languages: \" + str(len(languages.values())))\n",
    "print (\"Languages with max repetitions has:  \" + str(max(languages.values())))\n",
    "print (\"Discarded intances:  \" + str(dicarded))\n",
    "\n",
    "print (\"ALL\")\n",
    "print (all_languages)\n",
    "print (\"Number of languages: \" + str(len(all_languages.values())))\n",
    "print (\"Languages with max repetitions has:  \" + str(max(all_languages.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(languages, out_file)\n",
    "out_file.close()\n",
    "json.dump(all_languages, out_file_all)\n",
    "out_file_all.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Saving id's per language\")\n",
    "for id in en: en_ids.write(str(id) + '\\n')\n",
    "for id in tr: tr_ids.write(str(id) + '\\n')\n",
    "# for id in ca: ca_ids.write(str(id) + '\\n')\n",
    "en_ids.close()\n",
    "tr_ids.close()\n",
    "# ca_ids.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "lan_sorted = sorted(languages.items(), key=operator.itemgetter(1))\n",
    "lan_count_sorted = languages.values()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_ylabel('Number of images')\n",
    "ax.set_xlabel('Languages')\n",
    "ax.bar(languages.keys(),lan_count_sorted,color ='r',width=0.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(languages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot\n",
    "# lan_sorted = sorted(languages.items(), key=operator.itemgetter(1))\n",
    "# lan_count_sorted = languages.values()\n",
    "# # lan_count_sorted.sort(reverse=True) #??????????????????????????????????????????????????????????????????????????????\n",
    "# # sorted(lan_count_sorted,reverse=True)\n",
    "# topX = min(10, len(lan_count_sorted))\n",
    "# x = range(topX)\n",
    "# my_xticks = []\n",
    "# for l in range(0, topX):\n",
    "#     my_xticks.append(lan_sorted[-l - 1][0])\n",
    "# plt.xticks(x, my_xticks, size=11)\n",
    "# width = 1 / 1.5\n",
    "# plt.bar(x, lan_count_sorted[0], width, color=\"brown\", align=\"center\")\n",
    "# plt.title(\"Number of images per language\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot %\n",
    "# lan_sorted = sorted(languages.items(), key=operator.itemgetter(1))\n",
    "# lan_count_sorted = languages.values()\n",
    "# # lan_count_sorted.sort(reverse=True)\n",
    "# # sorted(lan_count_sorted,reverse=True)\n",
    "# topX = min(10, len(lan_count_sorted))\n",
    "# x = range(topX)\n",
    "# my_xticks = []\n",
    "# total = sum(lan_count_sorted)\n",
    "# for i, v in lan_count_sorted:\n",
    "#     lan_count_sorted[i] = v / total * 100\n",
    "\n",
    "# for l in range(0, topX):\n",
    "#     my_xticks.append(lan_sorted[-l - 1][0])\n",
    "# plt.xticks(x, my_xticks, size=11)\n",
    "# width = 1 / 1.5\n",
    "# plt.bar(x, lan_count_sorted[0:topX], width, color=\"brown\", align=\"center\")\n",
    "# plt.title(\"% of images per language\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot\n",
    "# lan_sorted = sorted(all_languages.items(), key=operator.itemgetter(1))\n",
    "# lan_count_sorted = all_languages.values()\n",
    "# # lan_count_sorted.sort(reverse=True)\n",
    "# # sorted(lan_count_sorted,reverse=True)\n",
    "# topX = min(20, len(lan_count_sorted))\n",
    "# x = range(topX)\n",
    "# my_xticks = []\n",
    "# for l in range(0, topX):\n",
    "#     my_xticks.append(lan_sorted[-l - 1][0])\n",
    "# plt.xticks(x, my_xticks, size=11)\n",
    "# width = 1 / 1.5\n",
    "# plt.bar(x, lan_count_sorted[0:topX], width, color=\"brown\", align=\"center\")\n",
    "# plt.title(\"Number of images per language\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot %\n",
    "# lan_sorted = sorted(all_languages.items(), key=operator.itemgetter(1))\n",
    "# lan_count_sorted = all_languages.values()\n",
    "# # lan_count_sorted.sort(reverse=True)\n",
    "# # sorted(lan_count_sorted,reverse=True)\n",
    "# topX = min(20, len(all_languages))\n",
    "# x = range(topX)\n",
    "# my_xticks = []\n",
    "# total = sum(lan_count_sorted)\n",
    "# for i, v in lan_count_sorted:\n",
    "#     lan_count_sorted[i] = v / total * 100\n",
    "\n",
    "# for l in range(0, topX):\n",
    "#     my_xticks.append(lan_sorted[-l - 1][0])\n",
    "# plt.xticks(x, my_xticks, size=11)\n",
    "# width = 1 / 1.5\n",
    "# plt.bar(x, lan_count_sorted[0:topX], width, color=\"brown\", align=\"center\")\n",
    "# plt.title(\"% of images per language\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print (\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
