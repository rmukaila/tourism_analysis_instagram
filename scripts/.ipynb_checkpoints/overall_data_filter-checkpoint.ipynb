{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from load_jsons import *\n",
    "import json\n",
    "from shutil import copyfile\n",
    "from langdetect import detect_langs\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories to work with\n",
    "\n",
    "city = 'antalya' #Note: you need to change this value to match the city (data) you currently dealing with\n",
    "filtered_jsons_dir = \"/home/emotionlex/Documents/newest/instaturkeydata/JSONS/filtered/\"+city+\"_filtered_jsons/\"\n",
    "output_file_path = \"/home/emotionlex/Documents/newest/instaturkeydata/JSONS/\"+city+\"_extracted_captions.json\"\n",
    "unfiltered_jsons_dir = \"/home/emotionlex/Documents/newest/instaturkeydata/JSONS/unfiltered/\"+city+\"_jsons\" \n",
    "\n",
    "txt_files_base_dir = \"/home/emotionlex/Documents/newest/instaturkeydata/PREPROCESSING/\"\n",
    "\n",
    "duplicated_images_blacklist_file_dir = txt_files_base_dir+city+\"_duplicated_blacklist.txt\"\n",
    "\n",
    "cities_file_dir = txt_files_base_dir+\"turkey_provinces.txt\"\n",
    "\n",
    "users_blacklist_file_dir = txt_files_base_dir+city+\"_blacklisted_users.txt\"\n",
    "\n",
    "overalfilteredList_file =txt_files_base_dir+city+\"_overall_filteredlist.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    data = {}\n",
    "    print (\"Loading data\")\n",
    "    c = 0\n",
    "    for file in glob.glob(path+ \"/*.json\"):\n",
    "        c += 1\n",
    "        if c % 50000 == 0:\n",
    "            print (c)\n",
    "        with open(file) as data_file:\n",
    "            try:\n",
    "                data[file.split('/')[-1][:-5]] = json.load(data_file)\n",
    "            except:\n",
    "                print (\"Failed decoding JSON, skipping\")\n",
    "                continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_TH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "50000\n",
      "Number of jsons: 60255\n"
     ]
    }
   ],
   "source": [
    "data = load(unfiltered_jsons_dir)\n",
    "print (\"Number of jsons: \" + str(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blacklisted imgs: 5479\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load duplicated blacklist\n",
    "duplicated_blacklist = []\n",
    "duplicated_blacklist_file = open(duplicated_images_blacklist_file_dir, \"r\")\n",
    "for line in duplicated_blacklist_file:\n",
    "    duplicated_blacklist.append(line.replace('\\n', ''))\n",
    "duplicated_blacklist_file.close()\n",
    "print(\"Blacklisted imgs: \" + str(len(duplicated_blacklist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blacklisted users: 9\n"
     ]
    }
   ],
   "source": [
    "# Load users blacklist\n",
    "users_blacklist = []\n",
    "users_blacklist_file = open(users_blacklist_file_dir, \"r\")\n",
    "for line in users_blacklist_file:\n",
    "    users_blacklist.append(line.replace('\\n', ''))\n",
    "print(\"Blacklisted users: \" + str(len(users_blacklist)))\n",
    "users_blacklist_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cities before blacklisting : 82\n",
      "\n",
      "Number of Blacklisted cities for antalya : 81\n"
     ]
    }
   ],
   "source": [
    "# City names to filter: We'll filter images containing other city names appart from the current city.\n",
    "#That will discard spam\n",
    "#Note that this file contains all Turkey cities so we have to first pop out the current city \n",
    "cities = []\n",
    "cities_file = open(cities_file_dir, \"r\")\n",
    "for line in cities_file:\n",
    "    cities.append(line.replace('\\n', '').lower())\n",
    "    \n",
    "# print('All cities in Turkey'+'\\n',cities)    \n",
    "print(\"Number of cities before blacklisting :\", str(len(cities)))\n",
    "print(\"\")\n",
    "cities.pop(cities.index(city))\n",
    "# print(\"Blacklisted cities for \"+city+\" : \"+'\\n', cities)\n",
    "print(\"Number of Blacklisted cities for \"+city+\" : \" + str(len(cities)))\n",
    "cities_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_by_user = 0\n",
    "discarded_by_city = 0\n",
    "discarded_by_short_caption = 0\n",
    "discarded_by_nul_caption = 0\n",
    "discarded_by_lan = 0\n",
    "discarded_by_duplicated = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of posts dicarded by duplicated: 500\n",
      "Num of posts dicarded by city: 5000\n",
      "Error detecting language, continuing\n",
      "\n",
      "Num of posts dicarded by duplicated: 1000\n",
      "Num of posts dicarded by city: 10000\n",
      "Num of posts dicarded by duplicated: 1500\n",
      "Error detecting language, continuing\n",
      "\n",
      "Num of posts dicarded by user: 500\n",
      "Num of posts dicarded by city: 15000\n",
      "Num of posts dicarded by duplicated: 2000\n",
      "Num of posts dicarded by short caption: 100 2343524866575982697\n",
      "Num of posts dicarded by duplicated: 2500\n",
      "Num of posts dicarded by city: 20000\n",
      "Num of posts dicarded by language: 1000\n",
      "Num of posts dicarded by duplicated: 3000\n",
      "Num of posts dicarded by city: 25000\n",
      "Num of posts dicarded by duplicated: 3500\n",
      "Num of posts dicarded by user: 1000\n",
      "Num of posts dicarded by city: 30000\n",
      "Num of posts dicarded by duplicated: 4000\n",
      "Num of posts dicarded by short caption: 200 2354533259990065081\n"
     ]
    }
   ],
   "source": [
    "output_data = {}\n",
    "\n",
    "c = 0\n",
    "count = 0\n",
    "for k, v in data.items():\n",
    "\n",
    "    c += 1\n",
    "    if c % 50000 == 0:\n",
    "        print (c)\n",
    "\n",
    "    # Discard by user\n",
    "    if v['owner']['id'] in users_blacklist:\n",
    "        discarded_by_user += 1\n",
    "        if discarded_by_user % 500 == 0:\n",
    "            print (\"Num of posts dicarded by user: \" + str(discarded_by_user))\n",
    "        continue\n",
    "\n",
    "    # Discard by duplicated\n",
    "    \n",
    "    if k in duplicated_blacklist:\n",
    "        discarded_by_duplicated += 1\n",
    "        if discarded_by_duplicated % 500 == 0:\n",
    "            print (\"Num of posts dicarded by duplicated: \" + str(discarded_by_duplicated))\n",
    "        continue\n",
    "\n",
    "    # Check if post has caption\n",
    "#     if 'node' not in v['edge_media_to_caption']['edges']:\n",
    "    no_captions = 0\n",
    "    discarded_no_caption = False\n",
    "    try:\n",
    "        if 'text' in v['edge_media_to_caption']['edges'][0]['node']:\n",
    "            discarded_no_caption = False\n",
    " \n",
    "    #note: when there is no caption(text) the above if statement will result in IndexOutOfRange error\n",
    "    #so you get to take that as a no caption by handling the exception appropriately as below\n",
    "    except:\n",
    "        discarded_no_caption = True\n",
    "        discarded_by_nul_caption += 1\n",
    "        if discarded_by_nul_caption % 500 == 0:\n",
    "            print (\"Num of posts dicarded by no caption: \" + str(discarded_by_nul_caption))\n",
    "    if discarded_no_caption:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Preprocess text: Here I only filter to be able to look for cities. The text processing will be done when training text models, because I want to save the captions as they are\n",
    "    caption = v['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "    caption = caption.replace('#', ' ')\n",
    "    caption = caption.lower()\n",
    "    words = caption.split()\n",
    "\n",
    "    # Check num of words. Discard if under the threshold\n",
    "    if len(words) < word_TH:\n",
    "        discarded_by_short_caption += 1\n",
    "        if discarded_by_short_caption % 100 == 0:\n",
    "            print (\"Num of posts dicarded by short caption: \" + str(discarded_by_short_caption),k)\n",
    "        continue\n",
    "\n",
    "    # Check if caption contains cities\n",
    "    cur_discarded = False\n",
    "    for province in cities:\n",
    "        \n",
    "#         caption.replace('#', ' ')\n",
    "        \n",
    "        if province in caption:\n",
    "            cur_discarded = True        \n",
    "            discarded_by_city += 1\n",
    "            if discarded_by_city % 5000 == 0:\n",
    "                print (\"Num of posts dicarded by city: \" + str(discarded_by_city))\n",
    "#             cur_discarded = True\n",
    "#             print(city)\n",
    "            break\n",
    "            \n",
    "    if cur_discarded==True:\n",
    "        continue\n",
    "\n",
    "#     #Check language\n",
    "# #     langs =''\n",
    "    try:\n",
    "        languages = detect_langs(caption)\n",
    "        languages_names = []\n",
    "        for l in languages:\n",
    "            languages_names.append(str(l)[:2])\n",
    "        \n",
    "        if 'en' not in languages_names[0] and 'tr' not in languages_names[0]:\n",
    "            discarded_by_lan += 1\n",
    "#             print('Discarded language(s): ',languages_names)\n",
    "            if discarded_by_lan % 1000 == 0:\n",
    "                print (\"Num of posts dicarded by language: \" + str(discarded_by_lan))\n",
    "            continue\n",
    "    except:\n",
    "        print (\"Error detecting language, continuing\")\n",
    "        discarded_by_lan += 1\n",
    "        print('')\n",
    "#         print('Discarded language(s): ',languages_names)\n",
    "        continue\n",
    "    \n",
    "    # Else save the data in a dir with keys the id and with values the captions (originals)\n",
    "    output_data[k] = v['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "\n",
    "     # And save the original filtered jsons in a separate folder\n",
    "    with open(overalfilteredList_file, 'a') as blk_list:\n",
    "        blk_list.write(k+'\\n')\n",
    "        \n",
    "# print('Number of total blacklisted users: ', len(blacklist_nulANDshortCaption_users_duplicated))        \n",
    "#     if count <10:\n",
    "#         with open(filtered_jsons_dir + k + '.json', 'w') as outfile_jsons:\n",
    "#             json.dump(v, outfile_jsons)\n",
    "#             count +=1\n",
    "#             if k=='2338585609239415007':\n",
    "#                 print(cities[cities.index('istanbul')] in caption)\n",
    "#             if count >= 10:\n",
    "#                 break\n",
    "#    #Save as jsons the id and captions that survived all the filtering\n",
    "#     outfile_jsons = open(filtered_jsons_dir + k + '.json', 'w') \n",
    "#     json.dump(v, outfile_jsons)\n",
    "#     outfile_jsons.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Discards: No captions: \" + str(discarded_by_nul_caption) + \" Short: \" + str(\n",
    "    discarded_by_short_caption) + \" City: \" + str(discarded_by_city) + \" User: \" + str(\n",
    "    discarded_by_user) + \" Language: \" + str(discarded_by_lan) + \" Duplicated: \" + str(discarded_by_duplicated))\n",
    "print (\"Number of original vs resulting elements: \" + str(len(output_data)) + \" vs \" + str(len(data)))\n",
    "\n",
    "print (\"Saving JSON\") #Creating a single json of all user id's and their captions\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    json.dump(output_data, outfile)\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
